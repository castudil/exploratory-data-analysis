{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMumgdpSaBjGhfv6b6+z7nX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/castudil/exploratory-data-analysis/blob/main/S7%20Linear%20Regression/LASSO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LASSO\n",
        "\n",
        "Lasso (short for Least Absolute Shrinkage and Selection Operator) is a linear regression method that, like ridge regression, adds a penalty term to the model's loss function to help prevent overfitting. However, the penalty term used in lasso is different than the one used in ridge regression.\n",
        "\n",
        "In lasso, the penalty term is the absolute value of the sum of the coefficients multiplied by a tuning parameter. This penalty term encourages the model to set some of the coefficients to exactly zero, effectively performing feature selection and making the model simpler and more interpretable.\n",
        "\n",
        "Lasso is often used in situations where there are many input variables, and it is not clear which variables are most important for predicting the output. By setting some of the coefficients to zero, lasso can effectively select the most important variables and discard the rest.\n",
        "\n",
        "Lasso can be a powerful tool for feature selection and creating interpretable models, but it may not perform as well as more complex models in situations where the relationships between the input variables and the output are highly nonlinear or complex."
      ],
      "metadata": {
        "id": "9ue5zGIFy7aI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import statsmodels.api as sm\n",
        "\n",
        "# Load the Boston Housing dataset from the R repository\n",
        "data = sm.datasets.get_rdataset('Boston', package='MASS').data\n",
        "\n",
        "# Separate the features (X) and the target variable (y)\n",
        "X = data.drop('medv', axis=1)\n",
        "y = data['medv']\n"
      ],
      "metadata": {
        "id": "Z2Un2iN00zQZ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use the get_rdataset() function from the statsmodels library to load the Boston Housing dataset from the R repository. We specify the name of the dataset ('Boston') and the package it belongs to ('MASS'). The function returns a Dataset object that we can access the data frame using the data attribute. Then, we extract the features (X) and the target variable (y) like in the previous examples."
      ],
      "metadata": {
        "id": "K-IyP0mx02K2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u63-PE5qy6W1",
        "outputId": "7cc8ab88-82e2-43ff-af13-66dbb4162b2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R^2 score: 0.6918147952283058\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Lasso regression object with alpha (tuning parameter) set to 0.1\n",
        "lasso = Lasso(alpha=0.1)\n",
        "\n",
        "# Fit the Lasso regression model to the training data\n",
        "lasso.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test data and calculate the R^2 score\n",
        "score = lasso.score(X_test, y_test)\n",
        "print(\"R^2 score:\", score)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we first load the Boston Housing dataset, which is a common dataset for regression problems. Then, we split the data into training and test sets using the train_test_split() function from scikit-learn.\n",
        "\n",
        "Next, we create a Lasso regression object using Lasso(alpha=0.1) and fit it to the training data using lasso.fit(X_train, y_train). The alpha parameter is the tuning parameter that controls the strength of the penalty term.\n",
        "\n",
        "Finally, we predict on the test data using lasso.predict(X_test) and calculate the R^2 score using lasso.score(X_test, y_test), which measures how well the model fits the data. A higher R^2 score indicates a better fit.\n",
        "\n",
        "Note that in practice, we would typically use cross-validation to choose the best value of alpha for the Lasso model, rather than setting it manually like we did in this example."
      ],
      "metadata": {
        "id": "vWrcOkqnzHUo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Standardize the features\n",
        "X = StandardScaler().fit_transform(X)\n",
        "\n",
        "# Initialize the Lasso model\n",
        "lasso = Lasso(alpha=0.1)\n",
        "\n",
        "# Fit the model\n",
        "lasso.fit(X, y)\n",
        "\n",
        "# Get the coefficients of the model\n",
        "coef = lasso.coef_\n",
        "\n",
        "# Print the coefficients and corresponding features\n",
        "for feature, coef in zip(data.columns[:-1], coef):\n",
        "    if coef != 0:  \n",
        "        print(feature, coef)\n",
        "    else:\n",
        "        print(\"*\",feature, coef)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_SztxdG1rrn",
        "outputId": "ac156258-900e-41ac-f41f-93daa269e758"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "crim -0.63230363899053\n",
            "zn 0.7084093141987329\n",
            "* indus -0.0\n",
            "chas 0.6576072255074015\n",
            "nox -1.574193347902239\n",
            "rm 2.8262690307935308\n",
            "* age -0.0\n",
            "dis -2.4220790078781755\n",
            "rad 1.1959368149844016\n",
            "tax -0.8464677789680638\n",
            "ptratio -1.9224934488824688\n",
            "black 0.7621653890824795\n",
            "lstat -3.7261838282515605\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we use the get_rdataset() function from statsmodels to load the Boston Housing dataset. We then standardize the features using StandardScaler from scikit-learn's preprocessing module. We set the target variable y to be the last column of the dataset.\n",
        "\n",
        "Next, we initialize the Lasso model with an alpha value of 0.1. We fit the model using the standardized features and target variable.\n",
        "\n",
        "Finally, we print the coefficients of the model along with their corresponding feature names. The features with non-zero coefficients are considered relevant by the Lasso model."
      ],
      "metadata": {
        "id": "2IoQqvBH5u1h"
      }
    }
  ]
}